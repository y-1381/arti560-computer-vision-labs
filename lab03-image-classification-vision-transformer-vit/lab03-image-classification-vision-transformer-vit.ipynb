{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfb003f",
   "metadata": {},
   "source": [
    "##### ARTI 560 - Computer Vision\n",
    "\n",
    "## Image Classification with Vision Transformer (ViT)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we build an image classification model to recognize five different flower categories from the TF Flowers dataset using a modern deep learning architecture called the Vision Transformer (ViT).\n",
    "\n",
    "Unlike traditional Convolutional Neural Networks (CNNs), Vision Transformers apply the Transformer concept (originally designed for Natural Language Processing) to images by splitting them into small patches and learning relationships between these patches using attention mechanisms. This approach has shown strong performance in many computer vision tasks.\n",
    "\n",
    "We will use a pretrained ViT model (trained on ImageNet) and adapt it to our flower dataset using transfer learning, which allows us to achieve good results with limited training time and a relatively small dataset.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook follows the steps below:\n",
    "\n",
    "1. **Load the Dataset**\n",
    "\n",
    "We load the TF Flowers dataset from TensorFlow Datasets (TFDS) and split it into:\n",
    "\n",
    "- Training set (70%)\n",
    "- Validation set (15%)\n",
    "- Test set (15%)\n",
    "\n",
    "2. **Preprocess the Images**\n",
    "\n",
    "Since the pretrained ViT model expects images of size **224×224**, we:\n",
    "\n",
    "- Resize all images to 224×224\n",
    "- Normalize pixel values to the range [0, 1]\n",
    "\n",
    "3. **Create Efficient Data Pipelines**\n",
    "\n",
    "We use the tf.data API to build efficient pipelines with:\n",
    "\n",
    "- Shuffling (for training)\n",
    "- Batching\n",
    "- Prefetching (to improve performance)\n",
    "\n",
    "4. **Apply Data Augmentation**\n",
    "\n",
    "To improve generalization, we apply augmentation layers such as:\n",
    "\n",
    "- Random horizontal flip\n",
    "- Random rotation\n",
    "- Random zoom\n",
    "\n",
    "\n",
    "5️. **Build a ViT Model with a Frozen Backbone**\n",
    "\n",
    "We load a pretrained Vision Transformer model using keras_hub and freeze the backbone so that:\n",
    "\n",
    "- The pretrained feature extractor remains unchanged\n",
    "- Only the classification head learns to adapt to the flower classes\n",
    "\n",
    "6️. **Train and Evaluate**\n",
    "\n",
    "Finally, we train the model for a few epochs and evaluate its performance on the unseen test set to report the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import keras_hub #provides pretrained models like: ViT, Bert\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "image_size = 224 #image size for the preset model\n",
    "batch_size = 32\n",
    "num_classes = 5\n",
    "epochs = 5\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad30ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder /root/tensorflow_datasets/tf_flowers/3.0.1 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/tf_flowers/3.0.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6d6b7eab0a4dd98d93764dede0c611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb98e77b47e476cbaf2616b03483134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8baddcbc23994a17bee27e4a19d00895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a2ad534e9a472486b54437c9f5791a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac54266eb05496582db69d57d11cd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/tf_flowers/incomplete.ACW1N5_3.0.1/tf_flowers-train.tfrecord*...:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Load TF Flowers dataset with proper splits\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:70%]\", \"train[70%:85%]\", \"train[85%:]\"],\n",
    "    as_supervised=True, # Returns (image, label) pairs.\n",
    "    with_info=True # Returns metadata like: number of samples, label names and image shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, [image_size, image_size])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # normalize to [0,1] and cast raw images from uint8 to float32\n",
    "    return image, label\n",
    "\n",
    "train_ds = (ds_train\n",
    "            .shuffle(1000)\n",
    "            .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)) #prepare the next batch on the CPU while the GPU is training the current batch - makes model training faster\n",
    "\n",
    "val_ds = (ds_val\n",
    "          .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .batch(batch_size)\n",
    "          .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "test_ds = (ds_test\n",
    "           .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "           .batch(batch_size)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "# Data augmentation layers\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.02),\n",
    "    layers.RandomZoom(0.2, 0.2),\n",
    "], name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/vit/keras/vit_base_patch16_224_imagenet/2/download/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593/593 [00:00<00:00, 1.19MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Build ViT model (frozen backbone)\n",
    "\n",
    "def vit_model(in_shape=(image_size, image_size, 3), num_classes=num_classes):\n",
    "    inputs = keras.Input(shape=in_shape)\n",
    "    x = data_augmentation(inputs)  # apply augmentation\n",
    "    # Pretrained ViT classifier\n",
    "    vit = keras_hub.models.ViTImageClassifier.from_preset(\n",
    "        \"vit_base_patch16_224_imagenet\",\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    vit.backbone.trainable = False  # freeze backbone\n",
    "    outputs = vit(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"vit_flowers_frozen\")\n",
    "    return model\n",
    "\n",
    "model = vit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3122409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vit_flowers_frozen\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vit_flowers_frozen\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vi_t_image_classifier           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │    <span style=\"color: #00af00; text-decoration-color: #00af00\">85,802,501</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ViTImageClassifier</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vi_t_image_classifier           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │    \u001b[38;5;34m85,802,501\u001b[0m │\n",
       "│ (\u001b[38;5;33mViTImageClassifier\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,802,501</span> (327.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m85,802,501\u001b[0m (327.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,845</span> (15.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,845\u001b[0m (15.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,798,656</span> (327.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m85,798,656\u001b[0m (327.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile model\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b4371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 587ms/step - accuracy: 0.2573 - loss: 2.0269 - val_accuracy: 0.5808 - val_loss: 1.1386\n",
      "Epoch 2/5\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 580ms/step - accuracy: 0.6306 - loss: 1.0656 - val_accuracy: 0.8022 - val_loss: 0.6890\n",
      "Epoch 3/5\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 593ms/step - accuracy: 0.7879 - loss: 0.6783 - val_accuracy: 0.8584 - val_loss: 0.4988\n",
      "Epoch 4/5\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 586ms/step - accuracy: 0.8414 - loss: 0.5264 - val_accuracy: 0.8820 - val_loss: 0.4012\n",
      "Epoch 5/5\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 603ms/step - accuracy: 0.8619 - loss: 0.4525 - val_accuracy: 0.8966 - val_loss: 0.3417\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bde7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 443ms/step - accuracy: 0.9193 - loss: 0.2910\n",
      "Final Test Accuracy: 0.918181836605072\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(\"Final Test Accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
