{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430e7f7b",
   "metadata": {},
   "source": [
    "##### ARTI 560 - Computer Vision  \n",
    "## Image Classification using Transfer Learning - Exercise \n",
    "\n",
    "### Objective\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Select another pretrained model (e.g., VGG16, MobileNetV2, or EfficientNet) and fine-tune it for CIFAR-10 classification.  \n",
    "You'll find the pretrained models in [Tensorflow Keras Applications Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "2. Before training, inspect the architecture using model.summary() and observe:\n",
    "- Network depth\n",
    "- Number of parameters\n",
    "- Trainable vs Frozen layers\n",
    "\n",
    "3. Then compare its performance with ResNet and the custom CNN.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- Which model achieved the highest accuracy?\n",
    "- Which model trained faster?\n",
    "- How might the architecture explain the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab09464",
   "metadata": {},
   "source": [
    "1. the selected pretrained model: EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a11fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9c5dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# loading cifar 10\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "class_names = [\n",
    "    \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
    "    \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"\n",
    "]\n",
    "\n",
    "y_train = y_train.squeeze().astype(\"int64\")\n",
    "y_test  = y_test.squeeze().astype(\"int64\")\n",
    "\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test  = x_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7078fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "], name=\"augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd43b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load efficientnetb0 and freeze\n",
    "eff_base = EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "eff_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd893c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cifar10_efficientnetb0\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cifar10_efficientnetb0\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resizing (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Resizing</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resizing (\u001b[38;5;33mResizing\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,062,381</span> (15.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,062,381\u001b[0m (15.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eff_model = keras.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    data_augmentation,\n",
    "    layers.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    layers.Lambda(preprocess_input),\n",
    "    eff_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10)   # logits\n",
    "], name=\"cifar10_efficientnetb0\")\n",
    "\n",
    "eff_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ce1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 148ms/step - accuracy: 0.6370 - loss: 1.0943 - val_accuracy: 0.8736 - val_loss: 0.3734 - learning_rate: 0.0010\n",
      "Epoch 2/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 149ms/step - accuracy: 0.7798 - loss: 0.6440 - val_accuracy: 0.8884 - val_loss: 0.3274 - learning_rate: 0.0010\n",
      "Epoch 3/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 150ms/step - accuracy: 0.7924 - loss: 0.5995 - val_accuracy: 0.8908 - val_loss: 0.3227 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# compile and train\n",
    "eff_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1),\n",
    "]\n",
    "\n",
    "history = eff_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b30f2c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetB0 (frozen) test accuracy: 0.8840000033378601\n",
      "EfficientNetB0 (frozen) test loss    : 0.3379216492176056\n"
     ]
    }
   ],
   "source": [
    "# test/eval\n",
    "test_loss_eff, test_acc_eff = eff_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"EfficientNetB0 (frozen) test accuracy:\", test_acc_eff)\n",
    "print(\"EfficientNetB0 (frozen) test loss    :\", test_loss_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544e5f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable layers in backbone: 30 / 238\n",
      "Epoch 1/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 179ms/step - accuracy: 0.7577 - loss: 0.7292 - val_accuracy: 0.8800 - val_loss: 0.3625\n",
      "Epoch 2/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 174ms/step - accuracy: 0.7992 - loss: 0.5869 - val_accuracy: 0.8896 - val_loss: 0.3225\n",
      "Epoch 3/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 174ms/step - accuracy: 0.8126 - loss: 0.5408 - val_accuracy: 0.8980 - val_loss: 0.2974\n",
      "EfficientNetB0 (fine-tuned) test accuracy: 0.8985000252723694\n",
      "EfficientNetB0 (fine-tuned) test loss    : 0.30153918266296387\n"
     ]
    }
   ],
   "source": [
    "eff_base.trainable = True\n",
    "\n",
    "for layer in eff_base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"trainable layers in backbone:\",\n",
    "      sum(l.trainable for l in eff_base.layers), \"/\", len(eff_base.layers))\n",
    "\n",
    "eff_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = eff_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss_ft, test_acc_ft = eff_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"EfficientNetB0 (fine-tuned) test accuracy:\", test_acc_ft)\n",
    "print(\"EfficientNetB0 (fine-tuned) test loss    :\", test_loss_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f497745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet frozen test acc: 0.8840000033378601\n",
      "EfficientNet fine-tuned test acc: 0.8985000252723694\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"EfficientNet frozen test acc\": float(test_acc_eff),\n",
    "    \"EfficientNet fine-tuned test acc\": float(test_acc_ft),\n",
    "}\n",
    "\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa11861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers in EfficientNetB0 backbone: 238\n",
      "Layers with learnable parameters (depth): 131\n",
      "0 normalization 7\n",
      "1 stem_conv 864\n",
      "2 stem_bn 128\n",
      "3 block1a_dwconv 288\n",
      "4 block1a_bn 128\n",
      "5 block1a_se_reduce 264\n",
      "6 block1a_se_expand 288\n",
      "7 block1a_project_conv 512\n",
      "8 block1a_project_bn 64\n",
      "9 block2a_expand_conv 1536\n",
      "10 block2a_expand_bn 384\n",
      "11 block2a_dwconv 864\n",
      "12 block2a_bn 384\n",
      "13 block2a_se_reduce 388\n",
      "14 block2a_se_expand 480\n",
      "15 block2a_project_conv 2304\n",
      "16 block2a_project_bn 96\n",
      "17 block2b_expand_conv 3456\n",
      "18 block2b_expand_bn 576\n",
      "19 block2b_dwconv 1296\n",
      "20 block2b_bn 576\n",
      "21 block2b_se_reduce 870\n",
      "22 block2b_se_expand 1008\n",
      "23 block2b_project_conv 3456\n",
      "24 block2b_project_bn 96\n",
      "25 block3a_expand_conv 3456\n",
      "26 block3a_expand_bn 576\n",
      "27 block3a_dwconv 3600\n",
      "28 block3a_bn 576\n",
      "29 block3a_se_reduce 870\n",
      "30 block3a_se_expand 1008\n",
      "31 block3a_project_conv 5760\n",
      "32 block3a_project_bn 160\n",
      "33 block3b_expand_conv 9600\n",
      "34 block3b_expand_bn 960\n",
      "35 block3b_dwconv 6000\n",
      "36 block3b_bn 960\n",
      "37 block3b_se_reduce 2410\n",
      "38 block3b_se_expand 2640\n",
      "39 block3b_project_conv 9600\n",
      "40 block3b_project_bn 160\n",
      "41 block4a_expand_conv 9600\n",
      "42 block4a_expand_bn 960\n",
      "43 block4a_dwconv 2160\n",
      "44 block4a_bn 960\n",
      "45 block4a_se_reduce 2410\n",
      "46 block4a_se_expand 2640\n",
      "47 block4a_project_conv 19200\n",
      "48 block4a_project_bn 320\n",
      "49 block4b_expand_conv 38400\n",
      "50 block4b_expand_bn 1920\n",
      "51 block4b_dwconv 4320\n",
      "52 block4b_bn 1920\n",
      "53 block4b_se_reduce 9620\n",
      "54 block4b_se_expand 10080\n",
      "55 block4b_project_conv 38400\n",
      "56 block4b_project_bn 320\n",
      "57 block4c_expand_conv 38400\n",
      "58 block4c_expand_bn 1920\n",
      "59 block4c_dwconv 4320\n",
      "60 block4c_bn 1920\n",
      "61 block4c_se_reduce 9620\n",
      "62 block4c_se_expand 10080\n",
      "63 block4c_project_conv 38400\n",
      "64 block4c_project_bn 320\n",
      "65 block5a_expand_conv 38400\n",
      "66 block5a_expand_bn 1920\n",
      "67 block5a_dwconv 12000\n",
      "68 block5a_bn 1920\n",
      "69 block5a_se_reduce 9620\n",
      "70 block5a_se_expand 10080\n",
      "71 block5a_project_conv 53760\n",
      "72 block5a_project_bn 448\n",
      "73 block5b_expand_conv 75264\n",
      "74 block5b_expand_bn 2688\n",
      "75 block5b_dwconv 16800\n",
      "76 block5b_bn 2688\n",
      "77 block5b_se_reduce 18844\n",
      "78 block5b_se_expand 19488\n",
      "79 block5b_project_conv 75264\n",
      "80 block5b_project_bn 448\n",
      "81 block5c_expand_conv 75264\n",
      "82 block5c_expand_bn 2688\n",
      "83 block5c_dwconv 16800\n",
      "84 block5c_bn 2688\n",
      "85 block5c_se_reduce 18844\n",
      "86 block5c_se_expand 19488\n",
      "87 block5c_project_conv 75264\n",
      "88 block5c_project_bn 448\n",
      "89 block6a_expand_conv 75264\n",
      "90 block6a_expand_bn 2688\n",
      "91 block6a_dwconv 16800\n",
      "92 block6a_bn 2688\n",
      "93 block6a_se_reduce 18844\n",
      "94 block6a_se_expand 19488\n",
      "95 block6a_project_conv 129024\n",
      "96 block6a_project_bn 768\n",
      "97 block6b_expand_conv 221184\n",
      "98 block6b_expand_bn 4608\n",
      "99 block6b_dwconv 28800\n",
      "100 block6b_bn 4608\n",
      "101 block6b_se_reduce 55344\n",
      "102 block6b_se_expand 56448\n",
      "103 block6b_project_conv 221184\n",
      "104 block6b_project_bn 768\n",
      "105 block6c_expand_conv 221184\n",
      "106 block6c_expand_bn 4608\n",
      "107 block6c_dwconv 28800\n",
      "108 block6c_bn 4608\n",
      "109 block6c_se_reduce 55344\n",
      "110 block6c_se_expand 56448\n",
      "111 block6c_project_conv 221184\n",
      "112 block6c_project_bn 768\n",
      "113 block6d_expand_conv 221184\n",
      "114 block6d_expand_bn 4608\n",
      "115 block6d_dwconv 28800\n",
      "116 block6d_bn 4608\n",
      "117 block6d_se_reduce 55344\n",
      "118 block6d_se_expand 56448\n",
      "119 block6d_project_conv 221184\n",
      "120 block6d_project_bn 768\n",
      "121 block7a_expand_conv 221184\n",
      "122 block7a_expand_bn 4608\n",
      "123 block7a_dwconv 10368\n",
      "124 block7a_bn 4608\n",
      "125 block7a_se_reduce 55344\n",
      "126 block7a_se_expand 56448\n",
      "127 block7a_project_conv 368640\n",
      "128 block7a_project_bn 1280\n",
      "129 top_conv 409600\n",
      "130 top_bn 5120\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of layers inside the EfficientNetB0 backbone\n",
    "print(\"Total layers in EfficientNetB0 backbone:\", len(eff_base.layers))\n",
    "\n",
    "# Filter only layers that actually have learnable parameters (weights/biases)\n",
    "trainable_layers = [layer for layer in eff_base.layers if layer.count_params() > 0]\n",
    "\n",
    "# Print the number of layers that contain learnable parameters (\"Depth of the Model\")\n",
    "print(\"Layers with learnable parameters (depth):\", len(trainable_layers))\n",
    "\n",
    "# Listing all layers that have learnable parameters\n",
    "# Each layer will be printed with:\n",
    "# (index in the filtered list, layer name, number of parameters)\n",
    "for i, layer in enumerate(trainable_layers):\n",
    "    print(i, layer.name, layer.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca05fe4",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "- Which model achieved the highest accuracy? Custom CNN (efficientnetb0 was close :))\n",
    "- Which model trained faster? Custom CNN\n",
    "- How might the architecture explain the differences? When using the custom CNN, it is much smaller and less complex than Resnet and Efficientnet. The size of the images remain as 32x32. the other two archtectures resize to 224x224 which requires more computation. Resnet and Efficientnet have millions of parameters whereas the custom CNN has fewer parameters and is lighter weight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
